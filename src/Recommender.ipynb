{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "import wals\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_map, item_map, train_sparse, test_sparse, unique_items, unique_users = model.clean_data(\"../data/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 1.3752278367431066, Test: 1.9272221487569356\n"
     ]
    }
   ],
   "source": [
    "latent_factors = 14\n",
    "num_iters = 20\n",
    "\n",
    "output_row, output_col = model.train_model(train_sparse, latent_factors, num_iters)\n",
    "\n",
    "train_rmse = wals.get_rmse(output_row, output_col, train_sparse)\n",
    "test_rmse = wals.get_rmse(output_row, output_col, test_sparse)\n",
    "print('Train: ' + str(train_rmse) + ', Test: ' + str(test_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610, 703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1066, 973, 920, 701, 792, 510]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This cell is just for testing...\n",
    "user = 18\n",
    "user_rated = [item_map[i] for i, x in enumerate(user_map) if x == user]\n",
    "# print(user_rated)\n",
    "print(str(output_row.shape[0]) + \", \" + str(len(user_rated)))\n",
    "model.generate_recommendations(user, user_rated, output_row, output_col, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "length: 609\n"
     ]
    }
   ],
   "source": [
    "# Generate top-n...\n",
    "k = 0\n",
    "topn_recommendations = []\n",
    "for u in unique_users:\n",
    "    user_rated = [item_map[i] for i, x in enumerate(user_map) if x == u]\n",
    "    if (k % 100 == 0):\n",
    "        print(k)\n",
    "    if user_rated:\n",
    "        topn_recommendations.append(model.generate_recommendations(u, [], output_row, output_col, 6))\n",
    "    k += 1\n",
    "print(\"length: \" + str(len(topn_recommendations)))\n",
    "with open(\"topn_100k.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(topn_recommendations, fp)\n",
    "    \n",
    "# ...or read them from a file...\n",
    "# with open(\"topn_100k.pickle\", \"rb\") as fp:\n",
    "    # topn_recommendations = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(topn_recommendations).transform(topn_recommendations)\n",
    "topn_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "frequent_itemsets = apriori(topn_df, min_support=0.05)\n",
    "frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "\n",
    "# frequent_itemsets[(frequent_itemsets['length'] == 2)]\n",
    "rules = association_rules(frequent_itemsets)\n",
    "\n",
    "rules = rules[(rules['support'] > 0.05) &\n",
    "      (rules['confidence'] > 0.2) &\n",
    "      (rules['lift'] > 3.0)]\n",
    "\n",
    "# print(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainableRecommendations = []\n",
    "for u in unique_users:\n",
    "    recommendations = []\n",
    "    user_rated = [item_map[i] for i, x in enumerate(user_map) if x == u]\n",
    "    if user_rated:\n",
    "        for index, row in rules.iterrows():\n",
    "            antecedents = list(row['antecedents'])\n",
    "            consequents = list(row['consequents'])\n",
    "            if all(x in user_rated for x in antecedents) and all(x not in user_rated for x in consequents):\n",
    "                recommendations.append({\"explanation\": tuple(row['antecedents']), \"recommendation\": tuple(row['consequents'])})\n",
    "    explainableRecommendations.append(recommendations)\n",
    "    \n",
    "with open(\"explainable_100k.pickle\", \"wb\") as fp:\n",
    "    pickle.dump(explainableRecommendations, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
