Model fidelity:
- Recommender system trained using 75%/25% train-test split
- Association rules were calculated with max. antecedent length of 3, consequent length of 1, min. support of 0.005 and min. confidence of 0.3
- To avoid computationally expensive operations, a 1% sample of all users (sampled using the modulus operator in natural user ordering) was used. This was done using global association rules only. For top-n calculation, movies users had rated before were allowed.

Results:
    n=20: 0.15422382671480145 (~15.4%) (redo!)
    n=30: (Peake, Wang): ?
    n=40: ?

MF-model
- Model was trained using the MovieLense 20m set of ratings. The data was first split with a 75%/25% train-test split. Training set was then used for cross-vadidation with 4 splits. Hyperparameters n_factors and n_epochs were optimised separately:

n_epochs (RMSE, MAE)
1: (0.8924, 0.6885)
2: (0.8791, 0.6778)
5: (0.8583, 0.6602)
10: (0.8288, 0.6347)
20: (0.8056, 0.6141)
35: (0.8087, 0.6150)
50: (0.8158, 0.6200)


- Model with n_factors = 100: 0.7894
- Model with n_factors = 50: 0.7896
- Model with n_factors = 10: 0.7971


Influence experiments

Exp 1 (check how variance in predicted influences changes)
- For 100 simulated users, rate 10 random movies
- Generate top-6 recommendations
- Test the influence of each rated movie 20 times (for each recommendation), track mean and standard deviation

At the end, plot mean of all means, and mean of all standard deviations
Mean of means: 0.22247165233384802, std of stds: 0.016411722250524757

Exp 2 (training with optimisation vs retraining)
- Used the development set to reduce computation time 
- For 50 simulated users, rate 20 random movies
- Train one model with original data, add user by optimisation ("influence")
- Train baseline by training with original data and user
- Train control in a way identical to baseline
- Get top-30 recommendations for each model, compare precision of control and influence (how many recommendations match with recommendations of the baseline)

Results:
Influence: mean precision=0.2813333333333333, std=0.07800226754073626
Control: mean precision=0.28733333333333333, std=0.0818964558357567

No significant differences were found between models

Limitations of both:
- Use random movies and ratings -- in reality some films are rated more, and average of ratings may not be 3.
- In second, used smaller data set -- may not be representative. Though, likely the optimisation would approximate retraining better in a bigger dataset